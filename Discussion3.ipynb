{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discussion3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DhJ4HOeEAgw",
        "colab_type": "text"
      },
      "source": [
        "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? \n",
        "\n",
        "As Evan Estola's presentation points out: if you let the machine take over the algorithm, they will perpetutate biases that exist. For example, he noted that females are much less likely to go to tech meetups. That's what the data shows and if their algorithms were to run free, then that's what would persist. However, it doesnt have to be that way. They perform their algorithms by separating the gender variable completely from the interests variable and combine them later, to ensure that no gendered assumptions are carried over, perpetuating the idea that women dont go to tech meetups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HRsKthYEHLn",
        "colab_type": "text"
      },
      "source": [
        "Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.\n",
        "\n",
        "When companies actively try to make recommender systems that are equitable, then that is certainly a step in the right direction. However, recommendation systems cannot account for all of our own social problems. For example, in Evan Estola's presentation he points out that Google recommends ads regarding rehabiliation or laywers (ads that are suggestive of criminal activity) if they have a \"Black Sounding Name\". Google did not take the time to remove prejudices, because prejudices are everywhere. Perhaps we can think of women in tech and help that cause, but what about minorities in tech? What about the fact that African Americans are incarcerated at more than 5 times the rate of whites? There are so many predjuices that would be so difficult to try and remove them all, that even with Meetup trying to do good, it would be a challenge to reverse everything."
      ]
    }
  ]
}